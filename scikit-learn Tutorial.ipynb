{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy \n",
      "import scipy \n",
      "import matplotlib \n",
      "import sklearn \n",
      "print(numpy.__version__) \n",
      "print(scipy.__version__) \n",
      "print(matplotlib.__version__) \n",
      "print(sklearn.__version__)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.7.1\n",
        "0.13.3\n",
        "1.2.1\n",
        "0.14.1\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training docs/images/sounds/transactions requires labels. We take the feature vectors and run them through a machine learning algorithm. We then take new training data and it classifies the new data.\n",
      "\n",
      "##Possible Applications\n",
      "\n",
      "- Text classification/Sequence tagging NLP (Spam filtering, sentiment analysis)\n",
      "- Computer vision / speech recognition (\"Measure the rates of hipsterness of restaurants by presence of mustaches\")\n",
      "- Learning to rank \u2013 IR and advertisement (Obvious.)\n",
      "- Science: Statistical analysis of the brain; Astronomy, biology, social sciences...\n",
      "\n",
      "##What's scikit learn?\n",
      "\n",
      "- Library of machine learning algorithms\n",
      "- Focus on standard methods (e.g., ESL-II)\n",
      "- Open Source (BSD)\n",
      "- Simple **fit/predict/transform** API\n",
      "- Python / NumPy / SciPy / Cython\n",
      "- Model Assessment, Selection & Ensembles \n",
      "\n",
      "*Grisel displays a 'vegetation cover type' data set as a matrix (variables of latitude, attitude, etc.). He's talking prediction.*\n",
      "\n",
      "##Support Vector Machine\n",
      "<code>from sklearn.svm import SVC\n",
      "\n",
      "model = SCV(kerynel = \"rbf\", C=1.0, gamma=1e-4)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "y_predicted = model.predict(X_test)\n",
      "\n",
      "from sklearn.metrics import f1_score\n",
      "f1_score(y_test, y_predicted)\n",
      "</code>\n",
      "\n",
      "So you could use an *SVM* to handle the task above. Or you could use a *linear classifier*. Comparing the support between the two is the way to determine the relative efficacy. *Random forests* would work, and it has very few parameters, so it's hard to mess up, which should appeal to everyone.\n",
      "\n",
      "*Grisel now displays a bunch of graphs on classifications of different models. SVM looks great but perhaps to the point of overfitting.*\n",
      "\n",
      "**Question:** What's the difference between AI and ML?\n",
      "\n",
      "ML is a part of AI. \"MI is the stuff that works.\" \n",
      "\n",
      "**Question:** Can this be used in an MR framework?\n",
      "\n",
      "Yes and no. Typically we pull features using Pig/Hive. Then we do sklearn, since the data set of features typically fits in memory.\n",
      "\n",
      "**An aside:** iPython puts input and output in one sheet, ready for distribution, which is why it's gathering such traction amongst analysts/programmers.\n",
      "\n",
      "**Question:** How does Pandas get column names?\n",
      "\n",
      "Pandas reads the header of the file by default.\n",
      "\n",
      "#scikit learn, part ii\n",
      "\n",
      "##Learning to Rank\n",
      "Bing example: Searched 'big data conference california,' returned 'Strata 2012' before 'Strata 2014.' *We want to display relevant content.*\n",
      "\n",
      "So how do we rank?\n",
      "\n",
      "- Input: numerical descriptors for query / results pairs\n",
      "- Target: relevance score (0: irrelevant, 1: somehwat relevant, 2: relevant)\n",
      "\n",
      "###Input\n",
      "Often input is PageRank, Click Through Rate, last update time...\n",
      "\n",
      "Could also be Query/Result page descriptors TF\\*IDF cosine similarity. Ratio of covered query terms, BM25 (Page descriptors)\n",
      "\n",
      "User context descriptors: past user interactions (clicks, +1), time (Y,M,D, more granular, even), user language\n",
      "\n",
      "Often search engines use more than 200 descriptors.\n",
      "\n",
      "###Quantifying Success\n",
      "- Measure discrepancy between predicted and true values\n",
      "- [NDCG (Normalized Discounted Cumulative Gain)](https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain)\n",
      "<code>\n",
      "In: ndcg([2, 4, 0, 1, ,1, 0, 0], rank=5)\n",
      "Out: 0.8625...\n",
      "</code>\n",
      "- You can use [Microsoft's public data set to get good with this](http://research.microsoft.com/en-us/projects/mslr/).\n",
      "\n",
      "*Grisel offers a disclaimer at this point\u2013it's only a couple of gigs, this file. It fits in RAM on a laptop. However, it could still benefit from distribution.*\n",
      "\n",
      "###Ranking with Decision Trees\n",
      "- We split our data set based on a particular decision threshold.\n",
      "- Training a Randomized Tree: Pick a random subset of features (TFIDF, BM25, PageRank, CTR...), find the feature that best splits the data\n",
      "- Train n trees. At prediction time, make each tree predict its best guess, and then make them guess, assess the guess.\n",
      "\n",
      "<code>\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "\n",
      "trees = ExtraTreesRegressor(n_estimators=100, n_jobs=8)\n",
      "trees.fit(X_train, y_train)\n",
      "\n",
      "</code>\n",
      "*Grisel uses R-squared to evaluate the model.*\n",
      "\n",
      "###Setting up a cluster for this processing\n",
      "- 10x8 cores cluster on EC2 in 20 min, using [StarCluster](https://github.com/jtriley/StarCluster) to set up the nodes.\n",
      "- A great demo of the way you can monitor the cluster's performance using iPython Notebook. iPython can also be used to schedule the jobs in parallel.\n",
      "\n",
      "###Summarizing the data\n",
      "- A big forest on a single node\u2013Copy the trees to a single list, return forest. \n",
      "- Whole process is here: http://nbviewer.ipython.org/github/ogrisel/notebooks/blob/master/Learning%20to%20Rank.ipynb\n",
      "\n",
      "http://nbviewer.ipython.org/ to display an iPython notebook\n",
      "\n",
      "I love the notion of comparing to a simple model every time. He's used a straight average (if we predicted death every time on the Titanic, we'd be right 66/100 times) and "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}